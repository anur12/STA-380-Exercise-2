---
title: "Exercise 2"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# **Intro to Predictive Modeling Exercise 2- Anuraag Mohile**

## Flights at ABIA

```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""} 
library(dplyr)
library(ggplot2)
library(ggthemes)
library(gridExtra)

f=read.csv("ABIA.csv")
f$Origin_Austin=0
f$Origin_Austin[f$Origin=='AUS']=1
  

#Creating a subset of flights leaving from Austin
orig_aus=f[f$Origin_Austin==1,]

#Removing NAs
na=orig_aus[complete.cases(orig_aus[ , "DepDelay"]),]

#Flights departing Austin delayed by more than 15 minutes
na0=na[na$DepDelay>15,]
```
Let's first see which months have the most delays
```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""} 

attach(na0)
#Grouping by month
del_month=as.data.frame(na0 %>% group_by(Month) %>% summarise(tot = n()))
ggplot(del_month, 
       aes(x = del_month$Month, y = del_month$tot))+geom_bar(stat = "identity", position = position_dodge(0.4), alpha = 0.4)+scale_x_discrete(limits= c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"))+xlab("Month")+ylab('Frequency')+ggtitle("Departure Delay from Austin > 15 minutes")
```

Now, destinations with the highest percentage of delayed flights and at least 100 delayed flights

```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""} 
na=orig_aus[complete.cases(orig_aus[ , "ArrDelay"]),]
na0=na[na$ArrDelay>15,]
#grouping by destination
del_dest=as.data.frame(na0 %>% group_by(na0$Dest) %>% summarise(tot = n()))
del_dest_all=as.data.frame(na %>% group_by(na$Dest) %>% summarise(tot = n()))

del_dest=del_dest[order(del_dest$tot,decreasing = TRUE),]
del_dest_all=del_dest_all[order(del_dest_all$tot,decreasing = TRUE),]

del_dest_all2=del_dest_all[(del_dest_all$`na$Dest` %in% del_dest$`na0$Dest`),]
del_dest_all2=del_dest_all2[order(del_dest_all2$tot,decreasing = TRUE),]
del_dest$Norm_delay=del_dest$tot/del_dest_all2$tot #Normalizing

del_dest=del_dest[order(del_dest$Norm_delay,decreasing = TRUE),]
del_dest2=del_dest[(del_dest$tot>100),] #Airports with Minimum 100 arrival delays
del_dest2=del_dest2[order(del_dest2$Norm_delay,decreasing = TRUE),]
del_dest2=head(del_dest2,5) #Top 5 worst destinations
ggplot(del_dest2, 
       aes(x = del_dest2$`na0$Dest`, y = del_dest2$Norm_delay))+geom_bar(stat = "identity", position = position_dodge(0.4), alpha = 0.4)+xlab("Airport")+ylab('Frequency')+ggtitle("Arrival Delay Destinations")
```

We now check the number of delayed flights by season for each of the top 5 destination airports with highest percentage of arrival delays.

```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""}
#Grouping by seasons based on the month
na$Season="Summer"
na$Season[na$Month %in% c(12,1,2)]="Winter"
na$Season[na$Month %in% c(3,4,5)]="Spring"
na$Season[na$Month %in% c(9,10,11)]="Fall"

na0=na[na$ArrDelay>15,]
del_season_dest=as.data.frame(na0 %>% group_by(na0$Dest,na0$Season) %>% summarise(tot = n()))

del_season_dest=del_season_dest[del_season_dest$`na0$Dest` %in% c("ATL","EWR","HOU","ORD","LAX"),]
ggplot(del_season_dest, 
       aes(x = del_season_dest$`na0$Dest`, y = del_season_dest$tot, fill = as.factor(del_season_dest$`na0$Season`))) +
     geom_bar(stat = "identity", position = position_dodge(0.85), alpha = 0.5) + 
  xlab("Airport")+ylab("Frequency")+
     scale_fill_discrete("Seasons", labels = waiver()) +
     ggtitle("Airport by season") + 
     scale_color_discrete("Seasons") +
     theme_minimal()
```

Finally, the percentage of flights with arrival delays of more than 15 minutes for each of the 5 airports, by season

```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""}
del_season_dest_all=as.data.frame(na %>% group_by(na$Dest,na$Season) %>% summarise(tot = n()))
del_season_dest_all=del_season_dest_all[del_season_dest_all$`na$Dest` %in% c("ATL","EWR","HOU","ORD","LAX"),]
del_season_dest$Normalized=del_season_dest$tot/del_season_dest_all$tot

 ggplot(del_season_dest, 
        aes(x = del_season_dest$`na0$Dest`, y = del_season_dest$Normalized, fill = as.factor(del_season_dest$`na0$Season`))) +
     geom_bar(stat = "identity", position = position_dodge(0.85), alpha = 0.5) + 
  xlab("Airport")+ ylab("Percentage of Flights Delayed By More Than 15 Minutes")+ 
     scale_fill_discrete("Seasons", labels = waiver()) +
     ggtitle("Worst Destinations By Season (Based on Arrival Delays)") + 
     scale_color_discrete("Seasons") +
     theme_minimal()
```

## Author attribution

```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""}
library(tm) 
library(magrittr)
library(slam)
library(proxy)
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }

# apply to all txt files in training set
file_list = Sys.glob('A:/MSBA/STA380-master/STA380-master/data/ReutersC50/C50train/*/*.txt')
s = lapply(file_list, readerPlain) 


y_train=file_list %>% 
{strsplit(.,'/',fixed = TRUE)} %>%
{lapply(., tail, n=2)} %>%
{lapply(., head, n=1)} %>%
  unlist

# Clean up the file names
mynames = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist

names(s) = mynames

# Create the training corpus, tokenize, preprocess, etc
corpus_train = Corpus(VectorSource(s))
corpus_train = tm_map(corpus_train, content_transformer(tolower))
corpus_train = tm_map(corpus_train, content_transformer(removeNumbers))
corpus_train = tm_map(corpus_train, content_transformer(removePunctuation))
corpus_train = tm_map(corpus_train, content_transformer(stripWhitespace))
corpus_train = tm_map(corpus_train, content_transformer(removeWords), stopwords("en"))
corpus_train = tm_map(corpus_train, content_transformer(removeWords), stopwords("SMART"))

DTM_train = DocumentTermMatrix(corpus_train)

# apply to all txt files in testing set
file_list_test = Sys.glob('A:/MSBA/STA380-master/STA380-master/data/ReutersC50/C50test/*/*.txt')
s_test = lapply(file_list_test, readerPlain)

y_test=file_list_test %>% 
{strsplit(.,'/',fixed = TRUE)} %>%
{lapply(., tail, n=2)} %>%
{lapply(., head, n=1)} %>%
  unlist

# Clean up the file names
mynames_test = file_list_test %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist

names(s_test) = mynames_test

# Create the test corpus, tokenize, preprocess, etc
corpus_test = Corpus(VectorSource(s_test))
corpus_test = tm_map(corpus_test, content_transformer(tolower))
corpus_test = tm_map(corpus_test, content_transformer(removeNumbers))
corpus_test = tm_map(corpus_test, content_transformer(removePunctuation))
corpus_test = tm_map(corpus_test, content_transformer(stripWhitespace))
corpus_test = tm_map(corpus_test, content_transformer(removeWords), stopwords("en"))
corpus_test = tm_map(corpus_test, content_transformer(removeWords), stopwords("SMART"))



# ignore words you haven't seen before
DTM_test2 = DocumentTermMatrix(corpus_test,
                               control = list(dictionary=Terms(DTM_train)))


# construct TF IDF weights
tfidf_train = weightTfIdf(DTM_train)
tfidf_test = weightTfIdf(DTM_test2)

library(tidyverse)
library(glmnet)
library(class)
```

### PCA
```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""}
set.seed(7)
# PCA on the TF-IDF weights
tfidf_train=tfidf_train[ , apply(tfidf_train, 2, var) != 0]
pc_author = prcomp(tfidf_train, scale=TRUE)
pve = summary(pc_author)$importance[3,]
plot(pve)

set.seed(7)
tfidf_test=tfidf_test[ , apply(tfidf_test, 2, var) != 0]
pc_author_test = prcomp(tfidf_test, scale=TRUE)

X_train= pc_author$x[,1:1000]
X_test= pc_author_test$x[,1:1000]
```

### KNN
```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""}
library(class)
TestPred.knn<-knn(X_train,X_test,y_train)

MisClassTest<-table("Predict"=TestPred.knn,"Actual"=y_test)
Accuracy= sum(diag(MisClassTest))/sum(MisClassTest)
Accuracy
```
Accuracy of knn is just
```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""}
Accuracy
```
### Multinomial Logistic Regression
```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""}
# Principal component regression using PCs
X = pc_author$x[,1:1000]
y=y_train

# Lasso cross validation
out1 = cv.glmnet(X, y, family='multinomial', type.measure="class")

# refit to the full data set with the "optimal" lambda
glm1 = glmnet(X, y, family='multinomial', lambda = 0.065)

#On test set
tfidf_test=tfidf_test[ , apply(tfidf_test, 2, var) != 0]
pc_author_test = prcomp(tfidf_test, scale=TRUE)
pve_test = summary(pc_author_test)$importance[3,]
X_test= pc_author_test$x[,1:1000]
fitted.results= as.data.frame(predict(glm1,X_test,type="response"))

pred=colnames(fitted.results)[apply(fitted.results,1,which.max)]
pred=pred %>% 
{strsplit(.,'.',fixed = TRUE)} %>%
{lapply(., head, n=1)} %>%
  unlist
pred=ifelse(pred==y_test,1,0)
Accuracy=mean(pred)
```

Accuracy of Logistic Regression is just
```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""}
Accuracy
```

The low accuracies show the challenge of text analysis.
Perhaps, adding a pseudo count for words not in training set would have improved accuracy.


## Practice with association rule mining
```{r,echo=FALSE,message=FALSE,warning=FALSE,comment=""}
library(tidyverse)
library(arules)  
library(arulesViz)
groc <- read.table("A:/MSBA/STA380-master/STA380-master/data/groceries.txt", header = FALSE, sep = ";", stringsAsFactors = FALSE)

item <- apply(groc, MARGIN = 1, FUN = function(x){unlist(strsplit(x, ","))})

# Remove duplicates ("de-dupe")
item <- lapply(item, unique)

item_transactions <- as(item, "transactions")

# apriori
groc_apriori <- apriori(item_transactions,
                        parameter=list(support = .001,
                                       confidence = .5))

plot(head(groc_apriori, n = 10, by ="lift"), method = "graph")
```


1) Popcorn, Soda and Salty snacks are bought together. These are all "snack" items.
2) Baking powder, Flour and Sugar are bought together. These are used in baking.
3) White Bread, Ham, Processed Cheese and eggs are bought together. These are breakfast items.
4) Dairy products like curd, yogurt, cream cheese and sour cream are bought together.
